{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ad8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student: Timur Carstensen\n",
    "# Student ID: 1722194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cfccc3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# IE 678 Deep Learning, University of Mannheim\n",
    "# Author: Rainer Gemulla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb3dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from IPython import get_ipython\n",
    "from helper import *\n",
    "from util import nextplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd20346",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use GPU if CUDA is available\n",
    "DATA_PATH = \"data/\"\n",
    "MODEL_PATH = \"data/\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d31d8ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To prevent from retraining models, you can save them to disk and load them\n",
    "# later on\n",
    "def save(model, filename):\n",
    "    torch.save(model.state_dict(), os.path.join(MODEL_PATH, filename))\n",
    "\n",
    "\n",
    "def load(model, filename):\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, filename)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63191e1f",
   "metadata": {},
   "source": [
    "# 1 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39cf82",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46adbc7f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training examples\n",
      "10000 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1. Modules/1. Master/1. MMDS/2. Semester/IE 678 - Deep Learning/4-Assignments/ie-678-deep-learning/dl22-a02/helper.py:47: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  images_as_tensors.append(torch.from_numpy(images).unsqueeze(1).float())\n"
     ]
    }
   ],
   "source": [
    "X, y, Xtest, ytest = load_dataset(\"fashionmnist\")\n",
    "class_dict = {\n",
    "    0: \"t-shirt/top\",\n",
    "    1: \"trouser\",\n",
    "    2: \"pullover\",\n",
    "    3: \"dress\",\n",
    "    4: \"coat\",\n",
    "    5: \"sandal\",\n",
    "    6: \"shirt\",\n",
    "    7: \"sneaker\",\n",
    "    8: \"bag\",\n",
    "    9: \"ankle boot\",\n",
    "}\n",
    "print(f\"{len(X)} training examples\")\n",
    "print(f\"{len(Xtest)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7be80de",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of example i=0: ankle boot\n",
      "Label of example i=1: t-shirt/top\n",
      "Label of example i=2: t-shirt/top\n",
      "Label of example i=3: dress\n",
      "Label of example i=4: t-shirt/top\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAAD8CAYAAADt0VN/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAedElEQVR4nO2de5DU5ZnvP++vL9NzYy4O14EAgVEgiCiGRQVUjAGTVZNgSWLiOcQ15lCJZmOsRTd/HE+lrLgxJyep0rNVniSultl111UTTTRqOORSAQRUDiDeGJjIzIAMw1yAmZ6+veePX7/PvN30XLt/Tv9If6umpud3ffuZ533e5/4qrTUlFB7ORA/gXEWJsB6hRFiPUCKsRygR1iOUCOsRPCOsUmqdUupdpdRBpdS9Xr2nWKG80GOVUgHgPeBaoBXYBXxJa32g4C8rUnjFscuBg1rrQ1rrGPAUcKNH7ypKBD16biNwxPq7FfiboS5WSvnN/DuhtZ483AVeEXZEKKXuAO6YqPfnib+MdIFXhG0DZll/z0wfE2itHwUeBV9y7IjwSsbuApqUUnOVUmHgi8DzHr2rKOEJx2qtE0qpbwIvAwHg51rrt7x4V7HCE3VrzIMooChQSpH9naqrq1m5ciUAL730Usa1gUAAgEQiMeTzDKznvq61vnS4cUzY4uUVHMchmUwCMH/+fABuv/12+vv7AThz5gzRaBSAnTt3nkVQpRSO48hn+7z5J5jnDzuOPL9HCUPgnOPYQCAgHLVmzRoAPvWpT9Ha2gpAWVkZFRUVAFx77bX89Kc/BeDDDz8E3Oluc2RVVRUAqVSKvr6+UY/jnCNsLBaTz5/85CcBmDNnjkxjx3F4+eWXAbj44ov5wQ9+AMDu3bsB2LdvH2+//TYAy5cvl2ds27aN7du3A9DT0zPiOEqiwCOcM1qBWb211lx77bUAwo21tbXE43HAndIGu3bt4uDBg0Amp0+fPh2AeDzOrl27ALjpppt45JFHANi6deuIWoGvCWurQgZaa3bs2AG4IiD72kQikUFEoyEYgr/xxhtC7EQiwbp16wD4+Mc/TmNjo7ltRMKWRIFH8PXiNdRs6+rqAgandH9/P2VlZQAEg0FZ6aPRKOXl5cAgx65atYrLL78ccBe6KVOmAPDb3/52TGMrcaxH8DXHDgWjpxoLynEc0UF7enro7OwEXBlsuN7IYMdx5P5kMimcPGuW7awbGb4mrE0Mo9RXVVUxY8YMAAYGBuS3EQWxWEyIXFtbK0Q2xAyHw5w6dQqAmpoa9u7dK8+99FJ3vTI673AoiQKP4GuONdPYNmM3bNjAtGnTAOjo6ACgvLxcpnRlZaVM61gsJpxs9NxgMCgL2nnnnSe669KlSwkGR08uXxPWfFFbL92/f7+IgFAoBGQSfsqUKaK7dnZ2yjWRSARwCW+0itbWVm655RYAHnroIdGPR4OSKPAIRcexZkEKBAIZftFcJmku5/SLL77ImTNnAMQHGw6HRWx0dHSIQyYSichzDeLxuLwjEAiwZMkSYHSOFxtFRVh7yg7l0bexevVqANavX88VV1wBQF9fn6z04XAYcEWGeW5fX58QtqysTESAIbztGgyHw5w+fRqAL3zhC7zwwguj/i4lUeARiopjs0Me9fX1AMyYMYOmpib5DC4HnX/++YCrpxqx0dfXx3nnnQdAe3s74JquhnunTJkii11FRQXbtm0DBh3aq1evFlHQ09MjomLFihVj+i5F5d1asWIF3/ve9wCYPHkytbW1gEtwM327u7sBV1QYpT4Wi4ls7u/vF0f1zTffDLgKfXV1NQB1dXUZXq9Dhw4ByPlTp06JOCgvLxeCT5o0Sd5Hybs1cRg3xyqlZgFPAFMBDTyqtf6JUqoe+HdgDtAC3Ky17hrhWToQCLB9+3bxSCWTyYwFx8BwrlnxDWpqagBoaGhg48aNAHz6058GYNOmTRli4fDhw4DLrUbEGPERi8VEt62urpbPqVSK2bNnm9d5yrEJ4Dta60XACuAbSqlFwL3AFq11E7Al/fdfHQomY5VSvwIeTv9cpbU+qpSaDvxea33BcPc2NDToG264gQcffJDm5mbAXUyMfDNmJwxaUzU1NRw54iY0tre3M3mym/znOI6YtJ/73OcAV181crWqqoply5YBsGzZMln0zILmOI4sdOnvJe81C9iRI0c+moQNpdQc4GLgNWCq1vpo+tQxXFExLBKJBMePH+fIkSOyiAwMDAjhqqqq5MtOmjQJgJMnT/KXv/xFzhvREI1GRQd+7rnnADfyaghbX18vROzu7pZV39yTSqUypr8hbDgcFi3EjGs45L14KaWqgGeAv9da99rntDsdck4JpdQdSqndSqndtq1/riAvjlVKhXCJ+gut9bPpwx8qpaZbouB4rnvtNM6Kigrd1taG1loSKyorK2loaABczjpx4gQw6LEKBoMiIkKhkFhQ1dXVMr3NPQsXLhQz98iRI+JkKSsrk2tszjWfy8vLRaz09PSwdOlSALZs2TIibcZNWOXOkZ8Bb2utf2Sdeh74r8CD6d+/GulZ/f397Nmzh2effZbbbrsNcOWm0TGj0ajIWzNNy8vLRTwEAgHxaCWTybPM06NHj8qxZDIpXjH7ubZ4MLpyPB4XETF37lzJlhkN8uHYK4BbgX1KqT3pY/+IS9D/UEr9HW7m8815vMO3KCrLC+C6664D4J577pEI6YkTJ4SLjG4bCAQynCxGv7XTOA13h0IhuTYUCmXkI5jPNjeaa1OplIiCvXv3iiXHKPTYoiGs4zgZLkGAq6++GoDvf//7QmRjCDiOI8S0vVcAx4+7Yt18t7a2Nnn26dOn5T77GiNX+/r6REa/+uqrYh4bn0IaJZN2olA0HDvaaxcsWAC4pqsRDzNnzqSlpQVwOc8YGR7CP6JgoscwRpREwUShRFiPUCKsRyiq0MxQyFViZGPBggU8/PDDADz99NO8+eabwKA1FY/HWbx4MQCf//znZXF76KGHZAEs+JiLdfEaipjGXv/iF7/I+vXrAddoqKysBFxT1zitc+G9994TnfaCCy4Qw8DUJfzwhz9k//79Iw25tHhNFIqWY21MmjSJJ554AkASKBzHkazAaDQqllMymcxwhoNbNGe4NPv7Gq+YydcKh8P86U9/AuDWW28dakjnhh77u9/9TuJNJhkjlUqJlyqRSGTY/9lRAduENedyjAFwCW/ibmvXruWdd97JdXlJFEwUilorMLGp2bNni0PacGkgEJBp3NjYmJHFbadkgise7NiV8bGeOnVKHOt2SpNx6Nx+++3cc8894xp7URPWeLfKysokWmAnrBnn9ubNmyW83draKtkyR4+6oTfHcUQslJWViXP7kksu4c477wTI+MeZd9x0003jJmxJFHiEol68TKLvlClTRAMwnFdVVSWplStWrJDkjMbGRh577DEAvv71rwNuMrJZ9QOBgOiue/bs4f333weQ50ciERELCxYsEMPivffes4fm734FF110EeAGAM1qbucYmFA4DNZhnTlzhkWLFgHINH7uuee4/vrrAXeqv/HGG4Arww0RjYFhV8p88MEHXHbZZcBZhB0RJVHgEYqWYxcvXiyh7kQikRHTAlehNzqtuR7cRA+jhz7wwANyj9EUlFLChTCY6mnqZG2O7e/vZ9WqVQA8/vjjYxp/iWM9QtFy7ObNm2XBOX36tOiW5pidSnTppZeK46W+vl5M2qlT3eymeDwulTLhcFjybjds2EBdXR0wmL1YU1OTUbtgiubGirwJm24QuRto01r/rVJqLm4vw/OA14Fb0/0Nx4Rt27ZJ6Hn+/PmyUJlF5v333xdi79ixQ6ZvKpXKCJGDu2AZEZJMJmUhPHXqlCxKxsCwi0ra29v55S9/OdahA4URBd8C3rb+/ifgf2mt5wNdwN8V4B2+Q156rFJqJvA48ABwN3A90AFMSzc1uwy4X2u9doTnDDuIuro6SRDetGkTAFdeeaVk/dXU1IjDOhQKZThdcrxLODIajYoHbN++fQB8+ctfHm4oBp7rsT8G/gGoTv99HtCttTaGdytuZ8680NXVxc6dO4HBwuM1a9aICzAcDouICAQCZyV+KKVEFKRSqYyCZeNvyErIyBvjFgVKqb8FjmutXx/n/ZLGOd4xFDPyTYq7QSn1GSACTAJ+AtQqpYJprj2rC6fBaLpx2h4pY8oaLu3t7c3o3GaLNNu3OhxskWHHvszxVCo14jOGwrg5Vmt9n9Z6ptZ6Dm63zf+rtf4ysBW4KX3ZqNI4h3kHWuuMsszm5maam5vp7e0lGAwSDAYzipS11hlT38A+FovFMoo4wP1H9fa6edOO4+A4zriJCt4YCJuBu5VSB3Fl7s88eEfRoyAGgtb698Dv058P4fboLhjsDhpGebd7DSQSCXFq29Fdw6G2JqC1lgWwoqIio21UQcdc0KeVIChak9aGLeuMKmUvWFrrjCChHWUwsPvHmPtSqVQGJ+d633jhW45tbGyUBSkQCMhCZ47l6iJnYBaneDye8YzhDIuxwreELXb4ThQY2ItNOByWxc3mVluftS0vo2YNDAycVa8w1PvGCl8QNhcGBgYy+mvnUurNsVgsJseCwWDObhrGlVgolESBR/Atx+ZytEDmNLab9RjYYkFrLSLFONCznzFe+Jaw2TlYuYgxFGFtFcsQ1uqeUZjxFfRpJQh8wbFDTc1ceqc91XNdly0qssM4hYIvCJsruzsWi+WcvqlU6qzdOIZLtc9F2L9qy6vY4QuOHQpmEbLTNI25ap+3O2XAIEfaC2ChRUGJYz2CLzg2l8xrb2+XHi2JRCIjr8Du6WJ+52oEYT+70DLWF4TNhdraWonMBoNBaXNiiwLb/jewu84dOXJEFsB58+bJNbYIGS9KosAj+IJjc6lLb775JgcOuNvcdnd3Z3Cn4TjTotTWbW2xEYvFJHfL5C1AfpwqYy7mjO6RYLpubNy4UerAjh07Nuw9S5culZ4HzzzzzFmNeUeJUjnSRMEXoiAXqqqquOGGGwC3gnDDhg2AW/1i8gzM7+rqaonozpw5k1/9yk11SCaTPP30056ML9+GZrXAT4HFuB3hbgPeZYzdOMeD06dPS3HHfffdx3e/+13ALcgwebGGmF1dXSJvX331VV588UVgsBmvF8hXFPwE+K3WegFwEW46Z6kbJ/l1iqsBVgMbAdLJxTGl1I3AVenLHsdN5NiczyCHgumP1d3dLf0K7rrrLknIMBzb3d3N66+7uXuPPfYYc+fOBQbb+XmBfETBXNxc2MeUUhfhZm9/i3F04xwvzPRuaGiQzpx33303M2fOBJDWp4cPH5ZCkIaGhoysGa+QjygIApcA/6y1vhg4Q9a0H203zjzGULTIh2NbgVat9Wvpv/8Tl7Bj7sY5Xj3WDoEbkxYG62KNTltRUZFRbmRn0HiFfNI4jwFHlFKmm/E1wAEGu3FCnmmcfka+euydwC/SO9IfAr6K+8/6SLpx2kFBOxIwXI6AbR6PZbejsSKvJ2ut9wC5TLtr8nnuaGH38DZ1XHYNQnY1I2R6v0z9gRcombQewbcmLZChNtmhmezcLftYIpEQji10OCZjbJ49+SOAvb+Mvbe3XYVoYGsAxoDwdGyev+GvFOcEx0JmCZHNvdkIBoPCscaf6wV8S9i6urqcfbmH6qtlNIVgMCgaRGVlpWgG5lihUBIFHsG3HDswMCBTOts0zf7bLkeCQbHR09NTcE418C1htdbjUpe01hkNe7xCSRR4BN9yrM2tdr2WDbNgZZ+zc74KkZyRCyWO9Qi+5dhIJJLhV83V8yWXDLZVM621hHcKvYj5lrDZ9Vy5+hUMd6/BUHpvviiJAo/gW44dTtXKLqu3P9tqWiAQ8MzZ7VvCKqXEe2XLWHtq56rots8rpaSLkemuUSiURIFH8C3HhkKhnAVyI21GAWRwuldWmG8Ja7fVM/0KRgM7ZB6Px0tagd/gW461dzvWWg9pvuaCvZ1foWtoDfLiWKXUt5VSbyml9iul/k0pFVFKzVVKvaaUOqiU+vd0zkHBEQ6HpV1JIpGQzyPBJnw8Hmf+/PnMnz+/4OPLpwVfI3AXcKnWejEQwG1sVurGSf4yNgiUK6WCQAVwFFiDm8cFbhrn5/J8R06YvQ4gkwtNFWK2WWsnapjziUSCjo4OT9I588ndagN+CHyAS9Ae3FTOgnfj9CPyEQV1wI24ebIzgEpg3RjuzyuNMxqNEgqFCIVCYqaan2QyKfW15icej0uFTCqVIpVKUVVVxZkzZ2Rf8EIiH63gU8BhrXUHgFLqWdwOnQXrxjkcdu7cKSWftbW10pov/TxgUGfNXtRM1/lkMjnm/Q1Gi3xk7AfACqVUhXK/iUnjLFg3Tj8j31bS/wPYACSAN4HbcWXqU0B9+thXtNbD5vSMN/HY5ARcffXVknhcWVl5ViMIyAzHtLW5k2jr1q0ZLaLGgHNjo7Qh7smpt9bX10s3envLFJPdfezYsYxowWib+GahVJk4UfCtSTt79myuuuoqAG688UapinnyySdlkx5TM7t+/XquucbNhe7r6+PJJ58E4NFHH/WsDsE3ouC6664D4Nvf/jbgNui1A4HV1W5j+8WLF0tlotloPZFIyKZpPT094ipsbGxky5YtgFsfNgaURMFEwRccO2/ePO6//34A2eSsoqIiI9nCaACzZs2S++zWJabuNpFIiKFw8uRJKVMyXeVHuZWfvzdKM/jOd75zlj3vOI6oW4lEQgh7+PBhIaI5b28qAYMRhGAwKBWNZtuqz372s/zmN7/Je8wlUeARfCEKli9fLouW4dyuri5ZsOwuGbFY7Kw6r97e3py5WbFYTKK0BoUSBSWO9Qi+kLE7d+5k+/btANJV47XXXpNki4qKCtFjY7GY1NIaC6uiokKu7e3tlapwcw7g3nsL21bBF6LARnNzMwB/+MMfRCykUikpsTfboMJgtkw8HhfChkIhIWZNTQ1bt24F4IUXXhjLkEuiYKLgC1EQDAZFnVq5ciUwuIMnuGaq3RLa+GYNl9olSHYYx3GcsXLq6MfsyVMLDNv9Z0zT5uZmaUESjUZFBKRSKZGtdmMzI1ftkk+jw3qBkijwCL7g2FxwHEf0WNuy6u3tPStL297vy+b+48dzNv8ozPg8e3KBYYevAVpbW+WY4zhS96W1liCjCSRGIhH6+/vlx2ywZiIJgBwr2HgL9qQSMuBbUdDS0iIcHA6HpatmS0uLTHezu3JXV5ccGxgYkPsKvTmaDd9ybH9/v+QHwGCuQCAQIBKJEIlE5FhXV5fkGhi5DLkb9xYKviVsscM3oiDbO2XyrsBd9bu6BvtSms9GGygvLxcNYPLkyWL+eokSx3qEETlWKfVzwOyavDh9rJ4crUzTGTE/AT4D9AEbtdZvFGKg2TWv1dXVsmD19fVRX18v1xrvlu1ssXVZk0swe/ZsOVbohWw0ouBfgIeBJ6xjppXpg0qpe9N/bwauA5rSP38D/HP6d97IFgUdHR3s378fyOwOH41GJUpriNnS0iLGQk1NjZjFdipooTGiKNBa/xE4mXX4RtzcV8jMgb0ReEK72IGbIDe9QGP1Fca7eA3VyrQROGJdZ/Jjj5IFpdQdwB3jfD+rVq3i0KFDgOtMMRzZ29srqUUm7NLf3y/cazINAaZNmyYNd8zi5jhOQUrs89YKtNZ6PLlXY0njtL+sCW8vWrRICFtbWytJcQcPHpSNJ4z3q7u7OyOPy+D06dPccsstAPz4xz8GCte3YLxawYdmime1Mm0DZlnXDZkfe65jvIQdqpXp88B/US5WAD2WyBg3bC5au3Yta9eulc0lwJ3+VVVVVFVV0dbWJo4ZY3m1traKk8XExsDVd6dOncrUqVMLXj0zGnXr33B7bjcopVqB/w48SO5Wpi/iqloHcdWtrxZspGksWbIEgL1790pMKxwOZyRkZFeG26ZvNBoVcdLb2yvFyXPmzAFcUVIIjEhYrfWXhjh1VivTdOvob+Q7qHMBvjFpDUcZHTQSiYhpasfE7G1QzbHsFCOTxT116lTxydoh8ULAN4T92Mc+BmS20jORgkgkkpGPZWAss0QikRFYPHz4MABNTU2SZGdUs/r6ek6ezFbbx46Sr8Aj+IZjszts9vX1iRkbCoXEALB3mzOtphOJhIS/Gxsb2b3bLS1bvXq1iBbD0XV1dSWOLWb4hmONZWXkakdHh+S0RiIRUZvC4bAsWiZaEA6HxeRdsmSJ5L92d3fL84w8LlRA0XeENaKgs7NTFpxgMChTOhwOi6PblHJm9zAw2kRXV5cshuba6dOn8+677+Y93pIo8Ai+4VizEBkd1ExdcEWBWbyCwaDopCZ0U1lZKccaGhpkR0+72aTtQC8EfEPYpqYmANFB7c0hHMfJcHRv27YNQDxXwWBQyo7sxI/a2loRAea5Jq0zX5REgVcwvVQm8gd3a6phf4LBoA4Gg/K34zjyed68eXJ+zpw5Iz6rAD+7R/pOvsvozhczZsygvb0938eUMronCr5ZvLKxZcsW0Qw6Ozv52te+BgzWz9qYMWOGLErl5eWScLxu3TpP2pZAiWM9g285NhAIiG46c+ZM9u3bB7hVM8888wwAX/nKV+RaY9LagUWvuBV8TNjOzk6JwnZ2dkomzLRp07jzzjsBuOiiiwDXP2DM3Oy4l1coiQKP4FvCHjp0SCKvxt86MDCQ0SGupaWFlpYWampqpELcFNB51SzSwLei4MCBAxnRWCMvY7GYRHIN+vv7heDBYLDgbaNzwbccW+wYbxrnQ8D1QAxoBr6qte5On7sPtwNnErhLa/2yFwNva2uTcnrHcSTt/ejRo9JsxxTVtbW1Zez9ZRpFeInRcOy/cHbPwleBxVrrJcB7wH0ASqlFuK1OP5G+538rpTzZ8bG9vV36FSqlJCkjGo1y4MABDhw4wLFjxzh27Ji0Lunp6RE57OW+3zDONE6t9StWx80duDla4KZxPqW1HtBaH8bNiFlewPH6BoVYvG7Dze4GN2Vzh3XOszanJ06ckCSOd955RwwApdRZcatYLJaxF43dkcMr5EVYpdR3cfsa/mIc9+aVH2ta6kGmjM21wUQoFBJiBoPBjEIQrzBuwiqlNuIuatfoQd/jqNM4821zWuwYF2GVUuuAfwCu1Frb7SyfB/5VKfUj3Ga9TcDOvEc5BOyNe3Ntf2LiWKZ4Lj32j0SPHW8a531AGfBqWnbt0Fr/N631W0qp/8DtI5sAvqG1TuZ+8rmN8aZx/myY6x8AHhjqfCGRK63d3tBnqN2RvPRqGfjWpIXcm0kopc5KPHYcR8RDMpn0dAdleafnb/grha85NtdUtzf3NTlctm6bSCRE//US5wxhh9qCysCIh2Qy+ZEQtiQKPIJvOfb888+XFMxUKpVhxmZzr+10SSQSkrnoJXzLsQsXLqS1tZXW1lbxcNkbpwUCgYxjJmcrHo9Lbdfll1/u2fh8S9hih28Je80110ielOM4OX2s9h5fhou11jQ3N9Pc3MymTZs8G59vZeyKFSvEY2WmPbjEzJXubuRuJBIRF+Nll13m2fh8y7HFDt9y7Jw5c8SvapusMKiz5sqkDAQCEvqeNm2aVCzanrJCoMSxHsF3hK2rq6Ouro6GhgZisRixWEwWJePZMp9zbZgWDod55ZVXeOWVV+jv72fZsmUsW7as4OP0nShYunQpkLn3t71Tkr0/gm1AmPOJRIILLrgAcMM0CxcuBJC6hULBdxzrF/iOY6+//nrAjdJm74EIbtmSUb1MgNGuXIzH47LfVyKR4MILL/RknL4jrKnRqq6uFgI5jiOFxdOmTRPi//rXvwbc3C2jCdhd5ysrK/nEJz7hyThLosAj+I5jDReaTdLAFQV2Z43sppCJRCKjBZ9Z9KLRqGSCFxq+LUfSWsu0PnHihEQLmpqauPLKKwH44x//CLhVh6ag+dSpU1LWWVdXJ07vMXaYL5UjTRR8JwoMLrzwwoxpbJuk2VHYqVOniqgIBoPCsWvXrvVsLwTfEnb//v2iVq1cuZJFixYBsGbNGv785z9nXPvII48IsZ966ileeuklz8dXEgUeoVgWrw7gDHBioscyBBrIHNtsrfWwjbqKgrAASqndI620E4XxjK0kCjxCibAeoZgI++hED2AYjHlsRSNjzzUUE8eeU5hwwiql1iml3lVKHUy3/p/IscxSSm1VSh1QSr2llPpW+vj9Sqk2pdSe9M9nRnzYBDfZCeBWNn4cCAP/D1g0geOZDlyS/lyNWxy4CLgfuGcsz5pojl0OHNRaH9Jax4CncIvwJgRa66M6vdOI1voU8DbjrFObaMIOtW/ChEMpNQe4GHgtfeibSqm9SqmfK6Xqhr7TxUQTtiihlKoCngH+Xmvdi7u1yzxgKe5mGf9zpGdMNGGLbt8EpVQIl6i/0Fo/C6C1/lBrndRap4D/wyjqgyeasLuAJqXUXKVUGLdy/PmJGkx6d6efAW9rrX9kHbf3y/k8sH+kZ02oP1ZrnVBKfRN4GVdD+LnW+q0JHNIVwK3APqXUnvSxfwS+pJRaitt+rwX4+kgPKlleHmGiRcE5ixJhPUKJsB6hRFiPUCKsRygR1iOUCOsRSoT1CP8f26TfWhAtrBMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(x):\n",
    "    \"Show one (or multiple) 28x28 MNIST images as a gray-scale image.\"\n",
    "    plt.imshow(x.reshape(-1, 28).cpu(), cmap=\"gray\", interpolation=\"none\")\n",
    "\n",
    "\n",
    "# Plot first 5 training examples. Each example consists of a 1x28x28 tensor with values\n",
    "# in [0,1] and a label\n",
    "nextplot()\n",
    "show_image(X[:5])\n",
    "for i in range(5):\n",
    "    print(f\"Label of example i={i}: {class_dict[y[i].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a474477",
   "metadata": {},
   "source": [
    "## 1a+b Implement a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3685e13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is a PyTorch version of logistic regression.\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x.float())\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cd9c7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement a simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create the required layers/function and store them as instance variables\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=1, out_channels=32, stride=1, padding=1, padding_mode=\"zeros\"),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=6272, out_features=10)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model output as a tensor of shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        # Use the layers/functions created above to compute model output\n",
    "        # YOUR CODE HERE\n",
    "        out = self.layer1(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.log_softmax(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583b8b99",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=6272, out_features=10, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# here is a description of what you created (this uses the member variables)\n",
    "print(SimpleCNN())\n",
    "\n",
    "# SimpleCNN(\n",
    "#   (layer1): Sequential(\n",
    "#     (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#     (1): Sigmoid()\n",
    "#     (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "#   )\n",
    "#   (fc1): Linear(in_features=6272, out_features=10, bias=True)\n",
    "#   (log_softmax): LogSoftmax()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5187ad14",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.0.weight',\n",
       "              tensor([[[[-3.1334e-01, -2.3116e-01,  2.0062e-02],\n",
       "                        [ 2.6935e-01,  2.5227e-01, -3.0663e-01],\n",
       "                        [ 2.5195e-01, -8.1105e-02,  2.7856e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1279e-01,  1.7968e-01,  2.5986e-01],\n",
       "                        [ 2.7267e-01, -1.4591e-01, -7.2891e-02],\n",
       "                        [ 3.2370e-01,  7.6536e-03,  4.4113e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2141e-01, -2.5207e-01,  9.8896e-02],\n",
       "                        [ 3.0507e-02,  2.3835e-01, -1.1050e-01],\n",
       "                        [ 1.6305e-02,  8.3803e-02, -8.0513e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2155e-01,  3.3898e-02, -9.3051e-02],\n",
       "                        [ 1.5562e-02,  1.5567e-02, -1.5731e-01],\n",
       "                        [-1.0225e-02, -5.5264e-02, -1.6012e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8450e-01,  2.5244e-01,  2.7358e-01],\n",
       "                        [ 4.7018e-02, -1.3220e-01,  1.0965e-01],\n",
       "                        [ 2.3343e-01,  3.1587e-01,  1.7759e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9508e-01,  2.4447e-01,  4.6551e-02],\n",
       "                        [ 1.2322e-01,  2.1611e-01, -3.0013e-01],\n",
       "                        [ 1.6477e-02,  1.3130e-01, -1.1931e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.3975e-02,  2.9238e-01, -2.4316e-01],\n",
       "                        [ 2.1296e-01,  3.0348e-01, -1.8219e-01],\n",
       "                        [ 3.8283e-02, -1.1460e-01,  2.3488e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0988e-01, -1.2149e-01,  1.6085e-01],\n",
       "                        [-2.1092e-01,  2.7231e-01, -4.7069e-02],\n",
       "                        [ 1.7326e-01, -2.0512e-01,  2.8040e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4611e-01,  2.0954e-01, -2.8068e-01],\n",
       "                        [ 8.9247e-02, -2.6699e-01,  2.9920e-01],\n",
       "                        [-1.0462e-01,  2.2788e-01, -8.3285e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0823e-02, -2.7114e-03,  3.2791e-01],\n",
       "                        [-2.8268e-01, -3.3148e-01, -3.2096e-01],\n",
       "                        [-2.4388e-01,  1.8833e-02,  2.8192e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.0051e-02, -2.9863e-01, -2.9226e-01],\n",
       "                        [-9.3239e-02, -1.1753e-01,  1.3877e-01],\n",
       "                        [ 7.9913e-02,  1.6093e-01, -1.3559e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3372e-01, -3.6394e-02,  8.0219e-02],\n",
       "                        [ 3.0546e-01, -2.2511e-01, -1.2078e-02],\n",
       "                        [-2.4778e-01, -1.2260e-01,  3.0276e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.8315e-02,  2.8724e-01,  2.5662e-01],\n",
       "                        [-3.0680e-01,  5.5820e-02,  4.2929e-02],\n",
       "                        [ 1.4462e-01, -5.2315e-02,  2.2823e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1448e-01,  2.8750e-01, -1.2811e-01],\n",
       "                        [-1.3768e-02, -1.4124e-01, -2.3031e-01],\n",
       "                        [ 1.3961e-01,  3.0354e-01,  2.5164e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0563e-01,  5.0319e-02,  1.3189e-01],\n",
       "                        [ 2.2655e-01, -3.3380e-02, -1.0648e-01],\n",
       "                        [ 1.6167e-01,  2.9223e-01,  3.8976e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4222e-01, -1.8620e-01, -9.9197e-02],\n",
       "                        [-2.2565e-01,  1.7153e-01,  2.1824e-01],\n",
       "                        [ 2.3171e-04,  1.8030e-01,  2.1655e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1468e-01, -1.1814e-01, -2.9796e-01],\n",
       "                        [ 2.3001e-03, -2.3055e-01,  1.9482e-01],\n",
       "                        [ 1.4367e-01, -2.8642e-01,  4.5965e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0761e-01,  9.5281e-02, -2.4420e-01],\n",
       "                        [-1.7872e-01, -2.3822e-01, -8.3466e-02],\n",
       "                        [-2.5124e-01,  3.5445e-02,  2.1049e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3797e-01, -1.1531e-01,  2.9048e-01],\n",
       "                        [-3.2197e-01,  2.6340e-01,  2.6496e-01],\n",
       "                        [-1.3993e-01, -3.1604e-01, -3.1449e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.9877e-01, -1.8188e-02,  1.9646e-02],\n",
       "                        [-1.3435e-02,  2.4335e-02,  2.4780e-01],\n",
       "                        [ 3.4066e-02, -5.1365e-02, -3.0279e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0934e-02,  3.4146e-03, -1.5632e-01],\n",
       "                        [ 2.0613e-01, -1.4886e-02, -6.5624e-02],\n",
       "                        [-2.9777e-01,  2.4502e-01, -1.4655e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1185e-01, -8.6104e-02,  2.5768e-01],\n",
       "                        [-9.6579e-02, -2.1872e-02,  3.0549e-01],\n",
       "                        [-8.8488e-02, -4.7774e-02, -1.6728e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8936e-02, -1.1413e-01,  3.2636e-01],\n",
       "                        [ 3.3042e-01,  1.3985e-01,  1.0645e-01],\n",
       "                        [-1.5490e-01, -2.6567e-01, -2.6780e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1376e-01,  4.6565e-02,  2.8166e-01],\n",
       "                        [ 2.2240e-01,  2.9677e-01,  1.6193e-01],\n",
       "                        [-9.7875e-02, -2.2337e-01,  1.6671e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.7657e-02,  1.7025e-01, -2.0547e-01],\n",
       "                        [-2.3673e-01, -1.7603e-01, -1.7582e-01],\n",
       "                        [-2.2511e-01, -2.4216e-01, -5.7653e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.1027e-02, -9.5678e-02,  3.1577e-01],\n",
       "                        [-1.1153e-01, -1.2067e-01,  2.6354e-01],\n",
       "                        [-2.0940e-01,  6.1597e-02, -7.1021e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9757e-02,  3.0808e-01, -9.3062e-02],\n",
       "                        [ 2.8657e-01, -3.3274e-01,  1.4531e-01],\n",
       "                        [ 1.7209e-02, -2.1124e-01, -1.5502e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.0937e-02,  1.6150e-01,  1.9201e-01],\n",
       "                        [ 1.5597e-01,  1.2375e-01,  2.6757e-01],\n",
       "                        [-5.9649e-02,  2.5684e-01, -2.1741e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2701e-01, -3.2451e-01,  2.9671e-01],\n",
       "                        [-1.8238e-01, -2.3288e-01, -3.0874e-01],\n",
       "                        [ 9.2656e-02, -1.1555e-01,  1.7193e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5057e-02, -1.5860e-01, -1.3265e-01],\n",
       "                        [ 1.6843e-01,  6.3929e-02, -2.2916e-01],\n",
       "                        [-3.1400e-01,  8.2236e-02,  3.0500e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3030e-01, -3.0515e-01, -1.3718e-01],\n",
       "                        [ 2.2962e-01,  2.6948e-01, -2.2388e-01],\n",
       "                        [ 2.4673e-01, -8.8586e-02,  1.8894e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2030e-01,  2.1542e-01, -3.0634e-01],\n",
       "                        [-1.3835e-01, -5.3805e-02, -1.3394e-01],\n",
       "                        [-2.8089e-01,  3.1296e-01,  1.6027e-01]]]])),\n",
       "             ('layer1.0.bias',\n",
       "              tensor([ 0.0372, -0.0484, -0.2134,  0.2302,  0.0640, -0.1612,  0.1082, -0.1007,\n",
       "                      -0.2888,  0.1670,  0.1086,  0.0870, -0.0457,  0.2410, -0.0297, -0.1837,\n",
       "                       0.2871,  0.1872, -0.1814,  0.1538,  0.1046, -0.2064,  0.3188,  0.0699,\n",
       "                       0.1407, -0.2702, -0.1232, -0.2408,  0.2301, -0.3057,  0.2963, -0.1979])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0032, -0.0113,  0.0060,  ...,  0.0052, -0.0063, -0.0036],\n",
       "                      [ 0.0022,  0.0118, -0.0020,  ..., -0.0030,  0.0041,  0.0089],\n",
       "                      [-0.0063, -0.0074,  0.0065,  ...,  0.0062, -0.0050, -0.0035],\n",
       "                      ...,\n",
       "                      [-0.0072,  0.0064, -0.0007,  ..., -0.0077,  0.0093, -0.0019],\n",
       "                      [ 0.0059,  0.0029, -0.0026,  ...,  0.0060, -0.0068,  0.0122],\n",
       "                      [ 0.0121, -0.0073,  0.0081,  ...,  0.0039, -0.0118,  0.0007]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0012, -0.0114,  0.0036, -0.0091,  0.0046,  0.0041, -0.0094, -0.0080,\n",
       "                       0.0113, -0.0061]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to see the parameters of your model is to look at its \"state\". Check that the\n",
    "# shapes of the parameters that you see here match your computations of task 1a).\n",
    "model = SimpleCNN().to(DEVICE)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12487ed5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0250, -2.6411, -1.9825, -3.4350, -2.4216, -2.1976, -2.9907, -2.2490,\n",
      "         -1.4162, -2.1895],\n",
      "        [-3.1033, -2.7851, -1.9185, -3.4815, -2.4930, -2.3651, -2.9073, -2.1501,\n",
      "         -1.3542, -2.2079],\n",
      "        [-2.9453, -2.9070, -2.0038, -3.6132, -2.3755, -2.2671, -2.9402, -2.3127,\n",
      "         -1.3658, -2.0606],\n",
      "        [-2.9246, -3.1919, -2.0326, -3.4886, -2.4006, -2.3286, -2.8503, -2.2359,\n",
      "         -1.3341, -2.0599],\n",
      "        [-3.0241, -2.8758, -1.9804, -3.5062, -2.2842, -2.3587, -2.7920, -2.3502,\n",
      "         -1.4004, -2.0599]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/pyn2nf8x3f94nbk8r749084r0000gn/T/ipykernel_49897/260300387.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.log_softmax(out)\n"
     ]
    }
   ],
   "source": [
    "# Run the forward pass on the first 5 examples.\n",
    "with torch.no_grad():  # tell torch to not compute backward graph\n",
    "    print(model(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f5ed93",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Labels: ['ankle boot', 't-shirt/top', 't-shirt/top', 'dress', 't-shirt/top']\n",
      "tensor([[[[ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  6.3730e-02,\n",
      "           -6.5272e-02, -6.4674e-02],\n",
      "          ...,\n",
      "          [ 1.9556e-01,  4.6598e-01,  2.0253e-01,  ...,  1.3041e+01,\n",
      "            7.8424e+00,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01]],\n",
      "\n",
      "         [[-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -2.0343e-01,\n",
      "           -1.3519e-01,  1.1519e-01],\n",
      "          ...,\n",
      "          [-7.0123e-01,  9.4528e-03, -8.5150e-02,  ..., -1.3269e+01,\n",
      "            2.6583e+00, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02]],\n",
      "\n",
      "         [[ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  6.6081e-01,\n",
      "            6.4063e-01,  3.7978e-01],\n",
      "          ...,\n",
      "          [-2.7053e-01, -4.1035e-01,  1.0987e-01,  ..., -3.6672e+01,\n",
      "           -1.4976e+01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -7.9104e-02,\n",
      "           -4.2418e-01, -5.3947e-01],\n",
      "          ...,\n",
      "          [ 1.0614e-01, -8.9015e-01, -3.0563e-01,  ..., -1.7298e+01,\n",
      "           -1.7257e+01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01]],\n",
      "\n",
      "         [[-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -3.0247e-01,\n",
      "           -2.1100e-01, -1.8414e-01],\n",
      "          ...,\n",
      "          [-4.1193e-01,  4.4953e-02, -2.8231e-01,  ...,  1.2158e+01,\n",
      "            9.2082e+00, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01]],\n",
      "\n",
      "         [[ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  7.5358e-01,\n",
      "            5.7092e-01,  3.7044e-01],\n",
      "          ...,\n",
      "          [ 2.0960e-01,  1.5265e-01,  2.8695e-01,  ..., -8.6038e+00,\n",
      "           -3.6077e+00,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0253e-01,  2.0253e-01,  6.4328e-02,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01, -5.2748e-02,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  1.4679e-01,  ..., -1.0753e+01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          ...,\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01]],\n",
      "\n",
      "         [[-8.5150e-02, -8.5150e-02,  4.6956e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02,  8.1533e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -9.1883e-02,  ...,  8.1287e+00,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          ...,\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02]],\n",
      "\n",
      "         [[ 1.0987e-01,  1.0987e-01,  3.9996e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01, -1.6069e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  5.9553e-02,  ...,  1.1177e+01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          ...,\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0563e-01, -3.0563e-01, -1.9440e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -1.0765e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -1.2242e-01,  ..., -9.8930e+00,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          ...,\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01]],\n",
      "\n",
      "         [[-2.8231e-01, -2.8231e-01, -2.7561e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -6.1121e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.3939e-01,  ...,  3.7426e+00,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          ...,\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01]],\n",
      "\n",
      "         [[ 2.8695e-01,  2.8695e-01,  5.5309e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  5.3856e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  5.9711e-01,  ...,  3.7101e+00,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          ...,\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          ...,\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01]],\n",
      "\n",
      "         [[-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          ...,\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02]],\n",
      "\n",
      "         [[ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          ...,\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          ...,\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01]],\n",
      "\n",
      "         [[-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          ...,\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01]],\n",
      "\n",
      "         [[ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          ...,\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          ...,\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01]],\n",
      "\n",
      "         [[-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          ...,\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02]],\n",
      "\n",
      "         [[ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          ...,\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          ...,\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01]],\n",
      "\n",
      "         [[-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          ...,\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01]],\n",
      "\n",
      "         [[ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          ...,\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          ...,\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01],\n",
      "          [ 2.0253e-01,  2.0253e-01,  2.0253e-01,  ...,  2.0253e-01,\n",
      "            2.0253e-01,  2.0253e-01]],\n",
      "\n",
      "         [[-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          ...,\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02],\n",
      "          [-8.5150e-02, -8.5150e-02, -8.5150e-02,  ..., -8.5150e-02,\n",
      "           -8.5150e-02, -8.5150e-02]],\n",
      "\n",
      "         [[ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          ...,\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01],\n",
      "          [ 1.0987e-01,  1.0987e-01,  1.0987e-01,  ...,  1.0987e-01,\n",
      "            1.0987e-01,  1.0987e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          ...,\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01],\n",
      "          [-3.0563e-01, -3.0563e-01, -3.0563e-01,  ..., -3.0563e-01,\n",
      "           -3.0563e-01, -3.0563e-01]],\n",
      "\n",
      "         [[-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          ...,\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01],\n",
      "          [-2.8231e-01, -2.8231e-01, -2.8231e-01,  ..., -2.8231e-01,\n",
      "           -2.8231e-01, -2.8231e-01]],\n",
      "\n",
      "         [[ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          ...,\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01],\n",
      "          [ 2.8695e-01,  2.8695e-01,  2.8695e-01,  ...,  2.8695e-01,\n",
      "            2.8695e-01,  2.8695e-01]]]])\n",
      "Predictions: ['coat', 'coat', 'coat', 'coat', 'coat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/pyn2nf8x3f94nbk8r749084r0000gn/T/ipykernel_46341/4250709550.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.log_softmax(out)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training (or test) examples, their correct labels, and the most likely model\n",
    "# predictions. Do not be worried if your (untrained) model always seems to predict the\n",
    "# same class.\n",
    "@torch.no_grad()\n",
    "def mnist_predict(model, start, end=None, use_test_data=False):\n",
    "    if end is None:\n",
    "        end = start + 1\n",
    "    if use_test_data:\n",
    "        images = Xtest[start:end].to(DEVICE)\n",
    "        labels = ytest[start:end].to(DEVICE)\n",
    "    else:\n",
    "        images = X[start:end].to(DEVICE)\n",
    "        labels = y[start:end].to(DEVICE)\n",
    "    nextplot()\n",
    "    show_image(images)\n",
    "    print(\"     Labels:\", [class_dict[label.item()] for label in labels])\n",
    "    out = model(images)\n",
    "    _, yhat = torch.max(out, 1)\n",
    "    print(\"Predictions:\", [class_dict[pred.item()] for pred in yhat])\n",
    "\n",
    "\n",
    "# first 5 examples from training + predictions\n",
    "mnist_predict(model, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1789bfc",
   "metadata": {},
   "source": [
    "## 1c Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b9967a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test model\n",
    "@torch.no_grad()\n",
    "def mnist_test(model, batch_size=100, reshape_1d=False):\n",
    "    \"\"\"\n",
    "    Function to test your CNN on test data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    model: trained CNN from task 1a\n",
    "    batch_size: size of batch for dataloader\n",
    "    reshape_1d: Reshape images to a 1d vectors (allows use of models other than CNNs\n",
    "                such as fully-connected FNNs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy of input model\n",
    "    \"\"\"\n",
    "    correct = 0  # number of correct predictions\n",
    "    total = 0  # total number of examples\n",
    "    model.eval()  # set layers like dropout and batch norm to eval mode\n",
    "\n",
    "    # Create test data loader\n",
    "    if reshape_1d:\n",
    "        dataset = torch.utils.data.TensorDataset(Xtest.reshape(len(Xtest), -1), ytest)\n",
    "    else:\n",
    "        dataset = torch.utils.data.TensorDataset(Xtest, ytest)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "\n",
    "    # Loop over data\n",
    "    for batch in test_loader:\n",
    "        # YOUR CODE HERE\n",
    "        # Update correct and total using the examples in the batch. To understand what a\n",
    "        # DataLoader does, have a look at the contents of \"batch\" before you start.\n",
    "        pred = list()\n",
    "        images, labels = batch\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # confusion matrix over classes\n",
    "        \n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy on {total} test images: {accuracy:.2f} %\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e15ecb3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10000 test images: 6.97 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code. Output should be: 6.97%\n",
    "class DummyModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(len(x), -1)[:, 200:210] * torch.arange(10).to(DEVICE)\n",
    "\n",
    "\n",
    "mnist_test(DummyModel().to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66a390",
   "metadata": {},
   "source": [
    "## 1d Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a45870",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "def mnist_train(\n",
    "    model, num_epochs=5, learning_rate=0.001, batch_size=100, reshape_1d=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to train the provided CNN network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: the model to train\n",
    "    num_epochs: number of epochs to train\n",
    "    learning_rate: learning rate to use\n",
    "    batch_size: size of batch for data loader\n",
    "    reshape_1d: Reshape images to a 1d vectors (allows use of models other than CNNs\n",
    "                such as fully-connected FNNs)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    if reshape_1d:\n",
    "        dataset = torch.utils.data.TensorDataset(X.reshape(len(X), -1), y)\n",
    "    else:\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    i = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7c524",
   "metadata": {},
   "source": [
    "## 1e Train and evaluate the simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bd730e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/pyn2nf8x3f94nbk8r749084r0000gn/T/ipykernel_46341/546750898.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.log_softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n",
      "torch.Size([100, 32, 14, 14])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1. Modules/1. Master/1. MMDS/2. Semester/IE 678 - Deep Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb Cell 23'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39m# Train a model.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000022?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m mnist_train(SimpleCNN()\u001b[39m.\u001b[39;49mto(DEVICE))\n",
      "\u001b[1;32m/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1. Modules/1. Master/1. MMDS/2. Semester/IE 678 - Deep Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb Cell 21'\u001b[0m in \u001b[0;36mmnist_train\u001b[0;34m(model, num_epochs, learning_rate, batch_size, reshape_1d)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000020?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000020?line=41'>42</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000020?line=42'>43</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000020?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000020?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1. Modules/1. Master/1. MMDS/2. Semester/IE 678 - Deep Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb Cell 12'\u001b[0m in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000011?line=38'>39</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpoolsigmoid(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000011?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000011?line=40'>41</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000011?line=42'>43</a>\u001b[0m \u001b[39m# out = torch.reshape(input=out, shape=(out.size(0),6272))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000011?line=43'>44</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(out, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/mtp-ai-turing-tumble/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a model.\n",
    "model = mnist_train(SimpleCNN().to(DEVICE))\n",
    "# save(model, \"simple_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3971f47f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/pyn2nf8x3f94nbk8r749084r0000gn/T/ipykernel_68590/2404266439.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.log_softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10000 test images: 85.12 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85.11999999999999"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a model. The simple CNN should perform much better after training.\n",
    "# model = load(SimpleCNN(), \"simple_cnn.pt\").to(DEVICE)\n",
    "mnist_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd4de6",
   "metadata": {},
   "source": [
    "## 1e Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e617a791",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try adding more layers, regularization, etc.\n",
    "class MyCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCnn, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8054d81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_cnn = MyCnn().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64380d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "mnist_train(my_cnn)\n",
    "# save(my_cnn, \"my_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f9c48",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# my_cnn = load(my_cnn, \"my_cnn.pt\")\n",
    "mnist_test(my_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc943bc8",
   "metadata": {},
   "source": [
    "# 2 Recurrent Neural Networks and Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf0642",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "facb8f04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load review and label data\n",
    "with open(os.path.join(DATA_PATH, \"reviews_small.txt\")) as f:\n",
    "    reviews_lines = f.readlines()\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"labels_small.txt\")) as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "print(reviews_lines[0])\n",
    "print(label_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d5c77e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high s satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i m here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn t\n",
      "First ten words:  ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n",
      "First label:  1\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations from the reviews and split words\n",
    "raw_reviews, words, labels = reviews_preprocess(reviews_lines, label_lines)\n",
    "print(raw_reviews[0])\n",
    "print(\"First ten words: \", words[:10])\n",
    "print(\"First label: \", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee5f695",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Determine an integer id for each unique word\n",
    "word_ids = reviews_create_word_ids(words)\n",
    "print(word_ids.get(\"the\"))\n",
    "print(word_ids.get(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ba6e34",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high s satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i m here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn t\n",
      "[10455, 307, 6, 3, 1177, 202, 8, 2217, 33, 1, 168, 56, 15, 49, 85, 8269, 43, 422, 122, 140, 15, 3151, 59, 144, 9, 1, 5230, 5946, 452, 72, 5, 260, 12, 10455, 307, 13, 2017, 6, 73, 2765, 5, 689, 76, 6, 3151, 1, 19565, 5, 1706, 6897, 1, 5947, 1707, 36, 52, 68, 211, 143, 63, 1390, 3151, 14816, 1, 19566, 4, 1, 221, 755, 31, 2710, 72, 4, 1, 5948, 10, 729, 2, 63, 1707, 54, 10, 208, 1, 321, 9, 64, 3, 1578, 3922, 737, 5, 2843, 187, 1, 422, 10, 1246, 9217, 33, 307, 3, 380, 322, 5949, 10, 135, 136, 5, 9218, 30, 4, 134, 3151, 1578, 2480, 5, 10455, 307, 10, 528, 12, 113, 1839, 4, 59, 676, 103, 12, 10455, 307, 6, 227, 4097, 48, 3, 2169, 12, 8, 231, 21]\n"
     ]
    }
   ],
   "source": [
    "# Encode each word in the review by its unique identifier\n",
    "encoded_reviews = reviews_encode(word_ids, raw_reviews)\n",
    "print(raw_reviews[0])\n",
    "print(encoded_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb122fb0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 10455   307     6     3  1177   202     8  2217    33     1   168    56\n",
      "    15    49    85  8269    43   422   122   140    15  3151    59   144\n",
      "     9     1  5230  5946   452    72     5   260    12 10455   307    13\n",
      "  2017     6    73  2765     5   689    76     6  3151     1 19565     5\n",
      "  1706  6897     1  5947  1707    36    52    68   211   143    63  1390\n",
      "  3151 14816     1 19566     4     1   221   755    31  2710    72     4\n",
      "     1  5948    10   729     2    63  1707    54    10   208     1   321\n",
      "     9    64     3  1578  3922   737     5  2843   187     1   422    10\n",
      "  1246  9217    33   307     3   380   322  5949    10   135   136     5\n",
      "  9218    30     4   134  3151  1578  2480     5 10455   307    10   528\n",
      "    12   113  1839     4    59   676   103    12 10455   307     6   227\n",
      "  4097    48     3  2169    12     8   231    21]\n"
     ]
    }
   ],
   "source": [
    "# Padding/truncating all reviews to the same length. Although this isn't strictly\n",
    "# necessary, it facilitates batch processing: all inputs of a batch need to have the\n",
    "# same length.\n",
    "sequence_length = 200\n",
    "padded_reviews = reviews_pad(encoded_reviews, sequence_length)\n",
    "print(padded_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793f145c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 400 400\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into 80% training, 10% test, and 10% validation Dataset\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = reviews_split(\n",
    "    padded_reviews, labels\n",
    ")\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942dd1a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data loaders for training\n",
    "train_loader, valid_loader, test_loader = reviews_create_dataloaders(\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b94355",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1. Modules/1. Master/1. MMDS/2. Semester/IE 678 - Deep Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb Cell 39'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=0'>1</a>\u001b[0m \u001b[39m# Here is an example how to use the train and test functions. Note that logistic\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=1'>2</a>\u001b[0m \u001b[39m# regression is a bogus model when used like this (since its assigns weights to\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=2'>3</a>\u001b[0m \u001b[39m# positions, but not word ids). So results will be bad.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression(sequence_length)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=4'>5</a>\u001b[0m reviews_train(model, train_loader, valid_loader, epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, device\u001b[39m=\u001b[39mDEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timurcarstensen/Library/CloudStorage/OneDrive-bwedu/1.%20Modules/1.%20Master/1.%20MMDS/2.%20Semester/IE%20678%20-%20Deep%20Learning/4-Assignments/ie-678-deep-learning/dl22-a02/a02.ipynb#ch0000038?line=5'>6</a>\u001b[0m reviews_test(model, test_loader, device\u001b[39m=\u001b[39mDEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Here is an example how to use the train and test functions. Note that logistic\n",
    "# regression is a bogus model when used like this (since its assigns weights to\n",
    "# positions, but not word ids). So results will be bad.\n",
    "model = LogisticRegression(sequence_length).to(DEVICE)\n",
    "reviews_train(model, train_loader, valid_loader, epochs=3, device=DEVICE)\n",
    "reviews_test(model, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635308d8",
   "metadata": {},
   "source": [
    "## 2a Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "152e66ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an LSTM for sentiment analysis\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_layers=1,\n",
    "        lstm_dropout_prob=0.5,\n",
    "        dropout_prob=0.3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size: number of unique words in the reviews\n",
    "        embeddings_dim: size of the embeddings\n",
    "        hidden_dim: dimension of the LSTM output\n",
    "        num_layers: number of LSTM layers\n",
    "        lstm_dropout_prob: dropout applied between the LSTM layers\n",
    "        dropout_prob: dropout applied before the fully connected layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embedding_dim\n",
    "            )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=lstm_dropout_prob\n",
    "            )\n",
    "        \n",
    "        self.dropout = nn.Dropout(\n",
    "            p=dropout_prob, \n",
    "            inplace=False\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=1\n",
    "            )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: batch as a (batch_size, sequence_length) tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Probability of positive class.\n",
    "        \"\"\"\n",
    "        # init hidden layer, which is needed for the LSTM\n",
    "        batch_size = len(x)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        # print(f\"shape of hidden represenation: {[x.shape for x in hidden]} \\n\")\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        embedding = self.embedding(x)     \n",
    "        # print(f\"shape of embedding represenation: {[x.shape for x in embedding]} \\n\")\n",
    "        \n",
    "        out, (hidden, cell_state) = self.lstm(embedding, hidden)\n",
    "        # print(f\"shape of lstm output: {[x.shape for x in out]} \\n\")\n",
    "        # print(f\"number of lstm outputs: {len(out)} \\n\")\n",
    "        # print(f\"shape of LSTM output: {out.shape} \\n\")\n",
    "        # print(f\"lstm out: {out} \\n\")\n",
    "        # print(f\"lstm hidden: {hidden} \\n\")\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # print(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Empty hidden LSTM state.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create two new tensors with sizes num_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters())  # only used to determine device\n",
    "\n",
    "        hidden = (\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "        )\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c3ed46",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTM(\n",
      "  (embedding): Embedding(1, 10)\n",
      "  (lstm): LSTM(10, 32, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test model setup\n",
    "lstm_model = SimpleLSTM(1, 10, 32, 2, 0, 0).to(DEVICE)\n",
    "print(lstm_model)\n",
    "\n",
    "# SimpleLSTM(\n",
    "#   (embedding): Embedding(1, 10)\n",
    "#   (lstm): LSTM(10, 32, num_layers=2, batch_first=True, dropout=0.0)\n",
    "#   (dropout): Dropout(p=0.0, inplace=False)\n",
    "#   (fc): Linear(in_features=32, out_features=1, bias=True)\n",
    "#   (sigmoid): Sigmoid()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba891c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
      "        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test forward function. \n",
    "# dummy data\n",
    "dummy_data = torch.zeros(train_loader.batch_size, sequence_length).long().to(DEVICE)\n",
    "# fix model parameters\n",
    "for key in lstm_model.state_dict():\n",
    "    lstm_model.state_dict()[key][:] = 0.1\n",
    "print(lstm_model(dummy_data).reshape(10, -1))\n",
    "# Output after reshape should be the following tensor\n",
    "#tensor([[0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643],\n",
    "#        [0.9643, 0.9643, 0.9643, 0.9643, 0.9643]], device='cuda or cpu',\n",
    "#       grad_fn=<ViewBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0da00",
   "metadata": {},
   "source": [
    "## 2b Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b04e781",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word_ids) + 1  # +1 for the 0 padding\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "# this may raise a warning when num_layers=1 (which is fine)\n",
    "# make sure to reinizialize the model if you want to train multiple times\n",
    "lstm_model = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, num_layers).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d10dceb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch:  1/ 5    Batch:  5   Batch loss: 0.710292    Val loss: 0.689997 Val acc: 0.515000\n",
      "Epoch:  1/ 5    Batch: 10   Batch loss: 0.670788    Val loss: 0.687631 Val acc: 0.545000\n",
      "Epoch:  1/ 5    Batch: 15   Batch loss: 0.705118    Val loss: 0.686999 Val acc: 0.532500\n",
      "Epoch:  1/ 5    Batch: 20   Batch loss: 0.696307    Val loss: 0.686578 Val acc: 0.547500\n",
      "Epoch:  1/ 5    Batch: 25   Batch loss: 0.673388    Val loss: 0.684610 Val acc: 0.547500\n",
      "Epoch:  1/ 5    Batch: 30   Batch loss: 0.692047    Val loss: 0.684232 Val acc: 0.557500\n",
      "Epoch:  1/ 5    Batch: 35   Batch loss: 0.706101    Val loss: 0.684290 Val acc: 0.552500\n",
      "Epoch:  1/ 5    Batch: 40   Batch loss: 0.678198    Val loss: 0.682825 Val acc: 0.545000\n",
      "Epoch:  1/ 5    Batch: 45   Batch loss: 0.695618    Val loss: 0.682859 Val acc: 0.562500\n",
      "Epoch:  1/ 5    Batch: 50   Batch loss: 0.685332    Val loss: 0.681706 Val acc: 0.550000\n",
      "Epoch:  1/ 5    Batch: 55   Batch loss: 0.673047    Val loss: 0.681690 Val acc: 0.552500\n",
      "Epoch:  1/ 5    Batch: 60   Batch loss: 0.676936    Val loss: 0.679366 Val acc: 0.542500\n",
      "96\n",
      "Finished epoch 1. Average batch loss: 0.6872616754844785. Average validation loss: 0.6843984872102737\n",
      "Starting epoch 2\n",
      "Epoch:  2/ 5    Batch: 65   Batch loss: 0.683470    Val loss: 0.678661 Val acc: 0.560000\n",
      "Epoch:  2/ 5    Batch: 70   Batch loss: 0.645485    Val loss: 0.676218 Val acc: 0.582500\n",
      "Epoch:  2/ 5    Batch: 75   Batch loss: 0.616327    Val loss: 0.675875 Val acc: 0.562500\n",
      "Epoch:  2/ 5    Batch: 80   Batch loss: 0.629467    Val loss: 0.668188 Val acc: 0.605000\n",
      "Epoch:  2/ 5    Batch: 85   Batch loss: 0.603317    Val loss: 0.670077 Val acc: 0.585000\n",
      "Epoch:  2/ 5    Batch: 90   Batch loss: 0.607771    Val loss: 0.668891 Val acc: 0.587500\n",
      "Epoch:  2/ 5    Batch: 95   Batch loss: 0.628573    Val loss: 0.662147 Val acc: 0.617500\n",
      "Epoch:  2/ 5    Batch: 100   Batch loss: 0.621446    Val loss: 0.666316 Val acc: 0.610000\n",
      "Epoch:  2/ 5    Batch: 105   Batch loss: 0.532221    Val loss: 0.644820 Val acc: 0.650000\n",
      "Epoch:  2/ 5    Batch: 110   Batch loss: 0.661708    Val loss: 0.656327 Val acc: 0.632500\n",
      "Epoch:  2/ 5    Batch: 115   Batch loss: 0.560296    Val loss: 0.658526 Val acc: 0.600000\n",
      "Epoch:  2/ 5    Batch: 120   Batch loss: 0.544663    Val loss: 0.663966 Val acc: 0.562500\n",
      "Epoch:  2/ 5    Batch: 125   Batch loss: 0.640561    Val loss: 0.661456 Val acc: 0.610000\n",
      "104\n",
      "Finished epoch 2. Average batch loss: 0.6077457461506128. Average validation loss: 0.6654974904197913\n",
      "Starting epoch 3\n",
      "Epoch:  3/ 5    Batch: 130   Batch loss: 0.485625    Val loss: 0.708251 Val acc: 0.610000\n",
      "Epoch:  3/ 5    Batch: 135   Batch loss: 0.464065    Val loss: 0.630289 Val acc: 0.657500\n",
      "Epoch:  3/ 5    Batch: 140   Batch loss: 0.490777    Val loss: 0.643368 Val acc: 0.635000\n",
      "Epoch:  3/ 5    Batch: 145   Batch loss: 0.544250    Val loss: 0.632635 Val acc: 0.637500\n",
      "Epoch:  3/ 5    Batch: 150   Batch loss: 0.460204    Val loss: 0.650867 Val acc: 0.600000\n",
      "Epoch:  3/ 5    Batch: 155   Batch loss: 0.496263    Val loss: 0.635614 Val acc: 0.647500\n",
      "Epoch:  3/ 5    Batch: 160   Batch loss: 0.539332    Val loss: 0.618583 Val acc: 0.642500\n",
      "Epoch:  3/ 5    Batch: 165   Batch loss: 0.576244    Val loss: 0.625226 Val acc: 0.652500\n",
      "Epoch:  3/ 5    Batch: 170   Batch loss: 0.507630    Val loss: 0.624644 Val acc: 0.667500\n",
      "Epoch:  3/ 5    Batch: 175   Batch loss: 0.545508    Val loss: 0.614249 Val acc: 0.682500\n",
      "Epoch:  3/ 5    Batch: 180   Batch loss: 0.454201    Val loss: 0.605970 Val acc: 0.662500\n",
      "Epoch:  3/ 5    Batch: 185   Batch loss: 0.399768    Val loss: 0.588855 Val acc: 0.712500\n",
      "Epoch:  3/ 5    Batch: 190   Batch loss: 0.547244    Val loss: 0.613527 Val acc: 0.657500\n",
      "104\n",
      "Finished epoch 3. Average batch loss: 0.5060007693246007. Average validation loss: 0.630159810758554\n",
      "Starting epoch 4\n",
      "Epoch:  4/ 5    Batch: 195   Batch loss: 0.528573    Val loss: 0.601785 Val acc: 0.667500\n",
      "Epoch:  4/ 5    Batch: 200   Batch loss: 0.359379    Val loss: 0.626283 Val acc: 0.647500\n",
      "Epoch:  4/ 5    Batch: 205   Batch loss: 0.308023    Val loss: 0.586697 Val acc: 0.707500\n",
      "Epoch:  4/ 5    Batch: 210   Batch loss: 0.676669    Val loss: 1.100777 Val acc: 0.560000\n",
      "Epoch:  4/ 5    Batch: 215   Batch loss: 0.363380    Val loss: 0.612647 Val acc: 0.705000\n",
      "Epoch:  4/ 5    Batch: 220   Batch loss: 0.448711    Val loss: 0.576349 Val acc: 0.715000\n",
      "Epoch:  4/ 5    Batch: 225   Batch loss: 0.440962    Val loss: 0.579091 Val acc: 0.695000\n",
      "Epoch:  4/ 5    Batch: 230   Batch loss: 0.412600    Val loss: 0.579181 Val acc: 0.687500\n",
      "Epoch:  4/ 5    Batch: 235   Batch loss: 0.482336    Val loss: 0.592404 Val acc: 0.695000\n",
      "Epoch:  4/ 5    Batch: 240   Batch loss: 0.301332    Val loss: 0.579172 Val acc: 0.720000\n",
      "Epoch:  4/ 5    Batch: 245   Batch loss: 0.353095    Val loss: 0.575055 Val acc: 0.705000\n",
      "Epoch:  4/ 5    Batch: 250   Batch loss: 0.379888    Val loss: 0.616041 Val acc: 0.682500\n",
      "Epoch:  4/ 5    Batch: 255   Batch loss: 0.418778    Val loss: 0.578219 Val acc: 0.722500\n",
      "104\n",
      "Finished epoch 4. Average batch loss: 0.44347705505788326. Average validation loss: 0.6310537755489349\n",
      "Starting epoch 5\n",
      "Epoch:  5/ 5    Batch: 260   Batch loss: 0.347637    Val loss: 0.595823 Val acc: 0.697500\n",
      "Epoch:  5/ 5    Batch: 265   Batch loss: 0.502298    Val loss: 0.614559 Val acc: 0.685000\n",
      "Epoch:  5/ 5    Batch: 270   Batch loss: 0.428178    Val loss: 0.580605 Val acc: 0.725000\n",
      "Epoch:  5/ 5    Batch: 275   Batch loss: 0.230405    Val loss: 0.574519 Val acc: 0.735000\n",
      "Epoch:  5/ 5    Batch: 280   Batch loss: 0.245947    Val loss: 0.595697 Val acc: 0.712500\n",
      "Epoch:  5/ 5    Batch: 285   Batch loss: 0.344496    Val loss: 0.595016 Val acc: 0.700000\n",
      "Epoch:  5/ 5    Batch: 290   Batch loss: 0.460641    Val loss: 0.607578 Val acc: 0.697500\n",
      "Epoch:  5/ 5    Batch: 295   Batch loss: 0.332908    Val loss: 0.583604 Val acc: 0.720000\n",
      "Epoch:  5/ 5    Batch: 300   Batch loss: 0.363302    Val loss: 0.575552 Val acc: 0.722500\n",
      "Epoch:  5/ 5    Batch: 305   Batch loss: 0.346483    Val loss: 0.586234 Val acc: 0.700000\n",
      "Epoch:  5/ 5    Batch: 310   Batch loss: 0.291196    Val loss: 0.584270 Val acc: 0.722500\n",
      "Epoch:  5/ 5    Batch: 315   Batch loss: 0.338883    Val loss: 0.584716 Val acc: 0.707500\n",
      "Epoch:  5/ 5    Batch: 320   Batch loss: 0.372010    Val loss: 0.595363 Val acc: 0.712500\n",
      "104\n",
      "Finished epoch 5. Average batch loss: 0.3474283532705158. Average validation loss: 0.5902718835725234\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate a model (without pretrained embeddings)\n",
    "n_epochs = 5\n",
    "# YOUR CODE HERE\n",
    "reviews_train(lstm_model, train_loader, valid_loader, epochs=5, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a724a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.695\n",
      "Test accuracy: 0.655\n"
     ]
    }
   ],
   "source": [
    "reviews_test(lstm_model, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b809908",
   "metadata": {},
   "source": [
    "#### training for 10 epochs to see if the model converges on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5db13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931a7a27",
   "metadata": {},
   "source": [
    "## 2c Load pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d87e5f4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reviews_load_embeddings(\n",
    "    embedding_layer, word_ids, pretrained_embeddings_file=\"data/word-embeddings.txt\"\n",
    "):\n",
    "    \"\"\"Load pretrained embeddings into an embedding layer.\n",
    "\n",
    "    Updates the weights of the embedding layer with with the embeddings given in the\n",
    "    provided word embeddings file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_layer: torch.nn.Embedding used in the model\n",
    "    word_ids: dictionary mapping each word to its unique identifier\n",
    "    pretrained_embeddings_file: path to the file containing pretrained embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Initializing embedding layer with pretrained word embeddings...\")\n",
    "    embeddings_index = dict()\n",
    "    words_initialized = 0\n",
    "    with open(pretrained_embeddings_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            encoded_word = word_ids.get(word)\n",
    "            if encoded_word is not None:\n",
    "                words_initialized += 1\n",
    "                embedding_layer.weight[encoded_word, :] = torch.from_numpy(\n",
    "                    np.asarray(values[1:], dtype=\"float32\")\n",
    "                )\n",
    "    print(\n",
    "        \"Initialized {}/{} word embeddings\".format(\n",
    "            words_initialized, embedding_layer.num_embeddings\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf574a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with pretrained word embeddings...\n",
      "Initialized 29841/32363 word embeddings\n",
      "tensor([[ 0.3825,  0.1482,  0.6060, -0.5153,  0.4399,  0.0611, -0.6272, -0.0254,\n",
      "          0.1643, -0.2210,  0.1442, -0.3721, -0.2168, -0.0890,  0.0979,  0.6561,\n",
      "          0.6446,  0.4770,  0.8385,  1.6486,  0.8892, -0.1181, -0.0125, -0.5208,\n",
      "          0.7785,  0.4872, -0.0150, -0.1413, -0.3475, -0.2959,  0.1028,  0.5719,\n",
      "         -0.0456,  0.0264,  0.5382,  0.3226,  0.4079, -0.0436, -0.1460, -0.4835,\n",
      "          0.3204,  0.5509, -0.7626,  0.4327,  0.6175, -0.3650, -0.6060, -0.7962,\n",
      "          0.3929, -0.2367, -0.3472, -0.6120,  0.5475,  0.9481,  0.2094, -2.7771,\n",
      "         -0.6022,  0.8495,  1.2549,  0.0179, -0.0419,  2.1147, -0.0266, -0.2810,\n",
      "          0.6812, -0.1417,  0.9925,  0.4988, -0.6754,  0.6417,  0.4230, -0.2791,\n",
      "          0.0634,  0.6891, -0.3618,  0.0537, -0.1681,  0.1942, -0.4707, -0.1480,\n",
      "         -0.5899, -0.2797,  0.1679,  0.1057, -1.7601,  0.0088, -0.8333, -0.5836,\n",
      "         -0.3708, -0.5659,  0.2070,  0.0713,  0.0556, -0.2976, -0.0727, -0.2560,\n",
      "          0.4269,  0.0589,  0.0911,  0.4728]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Try it\n",
    "test_embeddings = nn.Embedding(len(word_ids) + 1, 100).to(DEVICE)\n",
    "reviews_load_embeddings(test_embeddings, word_ids)\n",
    "print(test_embeddings(torch.LongTensor([word_ids.get(\"movie\")]).to(DEVICE)))\n",
    "del test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8320efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Glove Embedding: 29841\n"
     ]
    }
   ],
   "source": [
    "emmbed_dict = {}\n",
    "number_lines = 0\n",
    "with open(\"data/word-embeddings.txt\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:],'float32')\n",
    "        emmbed_dict[word]=vector\n",
    "        number_lines += 1\n",
    "print(f\"Number of words in Glove Embedding: {number_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aab35bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(emmbed_dict['in'])\n",
    "# each word is associated with a vector of length 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd3100",
   "metadata": {},
   "source": [
    "length of word ids is 32363 and the number of words in the embedding file is 29841; every word from the embedding file is used in the review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb32439",
   "metadata": {},
   "source": [
    "## 2d Train and evaluate with pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "773531d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit and evaluate a model with pretrained embeddings without fine-tuning\n",
    "class EmbeddingLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_layers=1,\n",
    "        lstm_dropout_prob=0.5,\n",
    "        dropout_prob=0.3,\n",
    "        finetuning=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size: number of unique words in the reviews\n",
    "        embeddings_dim: size of the embeddings\n",
    "        hidden_dim: dimension of the LSTM output\n",
    "        num_layers: number of LSTM layers\n",
    "        lstm_dropout_prob: dropout applied between the LSTM layers\n",
    "        dropout_prob: dropout applied before the fully connected layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.finetuning = finetuning\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embedding_dim\n",
    "            )\n",
    "\n",
    "        reviews_load_embeddings(embedding_layer=self.embedding, word_ids=word_ids)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=lstm_dropout_prob\n",
    "            )\n",
    "        \n",
    "        self.dropout = nn.Dropout(\n",
    "            p=dropout_prob, \n",
    "            inplace=False\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=1\n",
    "            )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: batch as a (batch_size, sequence_length) tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Probability of positive class.\n",
    "        \"\"\"\n",
    "        # init hidden layer, which is needed for the LSTM\n",
    "        batch_size = len(x)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self.embedding.weight.requires_grad = self.finetuning\n",
    "        embedding = self.embedding(x)        \n",
    "        out, (hidden, cell_state) = self.lstm(embedding, hidden)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Empty hidden LSTM state.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create two new tensors with sizes num_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters())  # only used to determine device\n",
    "\n",
    "        hidden = (\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "        )\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9ed9e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with pretrained word embeddings...\n",
      "Initialized 29841/32363 word embeddings\n",
      "Starting epoch 1\n",
      "Epoch:  1/ 5    Batch:  5   Batch loss: 0.710464    Val loss: 0.699767 Val acc: 0.512500\n",
      "Epoch:  1/ 5    Batch: 10   Batch loss: 0.684734    Val loss: 0.695903 Val acc: 0.500000\n",
      "Epoch:  1/ 5    Batch: 15   Batch loss: 0.680320    Val loss: 0.689206 Val acc: 0.552500\n",
      "Epoch:  1/ 5    Batch: 20   Batch loss: 0.677060    Val loss: 0.689100 Val acc: 0.532500\n",
      "Epoch:  1/ 5    Batch: 25   Batch loss: 0.683842    Val loss: 0.688320 Val acc: 0.535000\n",
      "Epoch:  1/ 5    Batch: 30   Batch loss: 0.697733    Val loss: 0.687791 Val acc: 0.517500\n",
      "Epoch:  1/ 5    Batch: 35   Batch loss: 0.681776    Val loss: 0.683834 Val acc: 0.567500\n",
      "Epoch:  1/ 5    Batch: 40   Batch loss: 0.700669    Val loss: 0.683577 Val acc: 0.567500\n",
      "Epoch:  1/ 5    Batch: 45   Batch loss: 0.704144    Val loss: 0.684707 Val acc: 0.552500\n",
      "Epoch:  1/ 5    Batch: 50   Batch loss: 0.662856    Val loss: 0.686202 Val acc: 0.537500\n",
      "Epoch:  1/ 5    Batch: 55   Batch loss: 0.643904    Val loss: 0.675397 Val acc: 0.585000\n",
      "Epoch:  1/ 5    Batch: 60   Batch loss: 0.674439    Val loss: 0.671828 Val acc: 0.592500\n",
      "96\n",
      "Finished epoch 1. Average batch loss: 0.6858562240377069. Average validation loss: 0.6863027562697729\n",
      "Starting epoch 2\n",
      "Epoch:  2/ 5    Batch: 65   Batch loss: 0.668603    Val loss: 0.693865 Val acc: 0.547500\n",
      "Epoch:  2/ 5    Batch: 70   Batch loss: 0.660033    Val loss: 0.664889 Val acc: 0.600000\n",
      "Epoch:  2/ 5    Batch: 75   Batch loss: 0.604151    Val loss: 0.653945 Val acc: 0.615000\n",
      "Epoch:  2/ 5    Batch: 80   Batch loss: 0.730161    Val loss: 0.697948 Val acc: 0.555000\n",
      "Epoch:  2/ 5    Batch: 85   Batch loss: 0.585495    Val loss: 0.648603 Val acc: 0.632500\n",
      "Epoch:  2/ 5    Batch: 90   Batch loss: 0.652392    Val loss: 0.684711 Val acc: 0.560000\n",
      "Epoch:  2/ 5    Batch: 95   Batch loss: 0.632004    Val loss: 0.659182 Val acc: 0.602500\n",
      "Epoch:  2/ 5    Batch: 100   Batch loss: 0.632115    Val loss: 0.647524 Val acc: 0.640000\n",
      "Epoch:  2/ 5    Batch: 105   Batch loss: 0.574239    Val loss: 0.734936 Val acc: 0.522500\n",
      "Epoch:  2/ 5    Batch: 110   Batch loss: 0.639121    Val loss: 0.645378 Val acc: 0.635000\n",
      "Epoch:  2/ 5    Batch: 115   Batch loss: 0.615341    Val loss: 0.629121 Val acc: 0.642500\n",
      "Epoch:  2/ 5    Batch: 120   Batch loss: 0.656933    Val loss: 0.633075 Val acc: 0.662500\n",
      "Epoch:  2/ 5    Batch: 125   Batch loss: 0.713260    Val loss: 0.693898 Val acc: 0.562500\n",
      "104\n",
      "Finished epoch 2. Average batch loss: 0.6548701152205467. Average validation loss: 0.6682365393409362\n",
      "Starting epoch 3\n",
      "Epoch:  3/ 5    Batch: 130   Batch loss: 0.669205    Val loss: 0.653647 Val acc: 0.610000\n",
      "Epoch:  3/ 5    Batch: 135   Batch loss: 0.665249    Val loss: 0.658751 Val acc: 0.605000\n",
      "Epoch:  3/ 5    Batch: 140   Batch loss: 0.644876    Val loss: 0.686030 Val acc: 0.597500\n",
      "Epoch:  3/ 5    Batch: 145   Batch loss: 0.612012    Val loss: 0.695821 Val acc: 0.520000\n",
      "Epoch:  3/ 5    Batch: 150   Batch loss: 0.622998    Val loss: 0.669833 Val acc: 0.595000\n",
      "Epoch:  3/ 5    Batch: 155   Batch loss: 0.601159    Val loss: 0.646938 Val acc: 0.662500\n",
      "Epoch:  3/ 5    Batch: 160   Batch loss: 0.620411    Val loss: 0.654736 Val acc: 0.620000\n",
      "Epoch:  3/ 5    Batch: 165   Batch loss: 0.680085    Val loss: 0.651984 Val acc: 0.630000\n",
      "Epoch:  3/ 5    Batch: 170   Batch loss: 0.602477    Val loss: 0.656282 Val acc: 0.622500\n",
      "Epoch:  3/ 5    Batch: 175   Batch loss: 0.537895    Val loss: 0.630419 Val acc: 0.682500\n",
      "Epoch:  3/ 5    Batch: 180   Batch loss: 0.615324    Val loss: 0.651419 Val acc: 0.667500\n",
      "Epoch:  3/ 5    Batch: 185   Batch loss: 0.583261    Val loss: 0.659497 Val acc: 0.612500\n",
      "Epoch:  3/ 5    Batch: 190   Batch loss: 0.646570    Val loss: 0.686390 Val acc: 0.565000\n",
      "104\n",
      "Finished epoch 3. Average batch loss: 0.6340719144791365. Average validation loss: 0.6616728873207018\n",
      "Starting epoch 4\n",
      "Epoch:  4/ 5    Batch: 195   Batch loss: 0.560752    Val loss: 0.688936 Val acc: 0.595000\n",
      "Epoch:  4/ 5    Batch: 200   Batch loss: 0.517095    Val loss: 0.638178 Val acc: 0.645000\n",
      "Epoch:  4/ 5    Batch: 205   Batch loss: 0.537687    Val loss: 0.646768 Val acc: 0.650000\n",
      "Epoch:  4/ 5    Batch: 210   Batch loss: 0.546199    Val loss: 0.638979 Val acc: 0.642500\n",
      "Epoch:  4/ 5    Batch: 215   Batch loss: 0.568716    Val loss: 0.664609 Val acc: 0.657500\n",
      "Epoch:  4/ 5    Batch: 220   Batch loss: 0.573746    Val loss: 0.702436 Val acc: 0.602500\n",
      "Epoch:  4/ 5    Batch: 225   Batch loss: 0.620118    Val loss: 0.639263 Val acc: 0.657500\n",
      "Epoch:  4/ 5    Batch: 230   Batch loss: 0.587699    Val loss: 0.670426 Val acc: 0.630000\n",
      "Epoch:  4/ 5    Batch: 235   Batch loss: 0.573462    Val loss: 0.623483 Val acc: 0.652500\n",
      "Epoch:  4/ 5    Batch: 240   Batch loss: 0.591770    Val loss: 0.630340 Val acc: 0.657500\n",
      "Epoch:  4/ 5    Batch: 245   Batch loss: 0.649969    Val loss: 0.635565 Val acc: 0.675000\n",
      "Epoch:  4/ 5    Batch: 250   Batch loss: 0.542431    Val loss: 0.611039 Val acc: 0.670000\n",
      "Epoch:  4/ 5    Batch: 255   Batch loss: 0.510202    Val loss: 0.611627 Val acc: 0.670000\n",
      "104\n",
      "Finished epoch 4. Average batch loss: 0.5851269084960222. Average validation loss: 0.6462807967685736\n",
      "Starting epoch 5\n",
      "Epoch:  5/ 5    Batch: 260   Batch loss: 0.606891    Val loss: 0.617336 Val acc: 0.655000\n",
      "Epoch:  5/ 5    Batch: 265   Batch loss: 0.554439    Val loss: 0.601137 Val acc: 0.675000\n",
      "Epoch:  5/ 5    Batch: 270   Batch loss: 0.642147    Val loss: 0.592333 Val acc: 0.690000\n",
      "Epoch:  5/ 5    Batch: 275   Batch loss: 0.660487    Val loss: 0.624754 Val acc: 0.660000\n",
      "Epoch:  5/ 5    Batch: 280   Batch loss: 0.505920    Val loss: 0.598490 Val acc: 0.692500\n",
      "Epoch:  5/ 5    Batch: 285   Batch loss: 0.516001    Val loss: 0.610428 Val acc: 0.692500\n",
      "Epoch:  5/ 5    Batch: 290   Batch loss: 0.561946    Val loss: 0.611621 Val acc: 0.670000\n",
      "Epoch:  5/ 5    Batch: 295   Batch loss: 0.647404    Val loss: 0.595710 Val acc: 0.685000\n",
      "Epoch:  5/ 5    Batch: 300   Batch loss: 0.590792    Val loss: 0.672498 Val acc: 0.655000\n",
      "Epoch:  5/ 5    Batch: 305   Batch loss: 0.616716    Val loss: 0.704118 Val acc: 0.567500\n",
      "Epoch:  5/ 5    Batch: 310   Batch loss: 0.560655    Val loss: 0.665855 Val acc: 0.637500\n",
      "Epoch:  5/ 5    Batch: 315   Batch loss: 0.524080    Val loss: 0.621026 Val acc: 0.662500\n",
      "Epoch:  5/ 5    Batch: 320   Batch loss: 0.599723    Val loss: 0.631545 Val acc: 0.650000\n",
      "104\n",
      "Finished epoch 5. Average batch loss: 0.5907629285939038. Average validation loss: 0.6266808314965322\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate a model with pretrained embeddings without fine-tuning.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "vocab_size = len(word_ids) + 1  # +1 for the 0 padding\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "embedding_model = EmbeddingLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, finetuning=False).to(DEVICE)\n",
    "reviews_train(embedding_model, train_loader, valid_loader, epochs=5, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f429fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.603\n",
      "Test accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "reviews_test(embedding_model, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53110e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with pretrained word embeddings...\n",
      "Initialized 29841/32363 word embeddings\n",
      "Starting epoch 1\n",
      "Epoch:  1/ 5    Batch:  5   Batch loss: 0.692048    Val loss: 0.683617 Val acc: 0.567500\n",
      "Epoch:  1/ 5    Batch: 10   Batch loss: 0.686108    Val loss: 0.676770 Val acc: 0.585000\n",
      "Epoch:  1/ 5    Batch: 15   Batch loss: 0.681123    Val loss: 0.681413 Val acc: 0.547500\n",
      "Epoch:  1/ 5    Batch: 20   Batch loss: 0.699391    Val loss: 0.675977 Val acc: 0.582500\n",
      "Epoch:  1/ 5    Batch: 25   Batch loss: 0.674776    Val loss: 0.673860 Val acc: 0.607500\n",
      "Epoch:  1/ 5    Batch: 30   Batch loss: 0.668833    Val loss: 0.675180 Val acc: 0.572500\n",
      "Epoch:  1/ 5    Batch: 35   Batch loss: 0.652374    Val loss: 0.671089 Val acc: 0.565000\n",
      "Epoch:  1/ 5    Batch: 40   Batch loss: 0.659105    Val loss: 0.709520 Val acc: 0.557500\n",
      "Epoch:  1/ 5    Batch: 45   Batch loss: 0.652515    Val loss: 0.665236 Val acc: 0.592500\n",
      "Epoch:  1/ 5    Batch: 50   Batch loss: 0.640147    Val loss: 0.671800 Val acc: 0.602500\n",
      "Epoch:  1/ 5    Batch: 55   Batch loss: 0.657240    Val loss: 0.659176 Val acc: 0.582500\n",
      "Epoch:  1/ 5    Batch: 60   Batch loss: 0.669158    Val loss: 0.655088 Val acc: 0.622500\n",
      "96\n",
      "Finished epoch 1. Average batch loss: 0.665737422183156. Average validation loss: 0.6748937026908001\n",
      "Starting epoch 2\n",
      "Epoch:  2/ 5    Batch: 65   Batch loss: 0.797063    Val loss: 0.705184 Val acc: 0.550000\n",
      "Epoch:  2/ 5    Batch: 70   Batch loss: 0.614427    Val loss: 0.640262 Val acc: 0.652500\n",
      "Epoch:  2/ 5    Batch: 75   Batch loss: 0.593997    Val loss: 0.643675 Val acc: 0.607500\n",
      "Epoch:  2/ 5    Batch: 80   Batch loss: 0.536356    Val loss: 0.638646 Val acc: 0.610000\n",
      "Epoch:  2/ 5    Batch: 85   Batch loss: 0.512985    Val loss: 0.668519 Val acc: 0.605000\n",
      "Epoch:  2/ 5    Batch: 90   Batch loss: 0.553879    Val loss: 0.625158 Val acc: 0.627500\n",
      "Epoch:  2/ 5    Batch: 95   Batch loss: 0.468676    Val loss: 0.615551 Val acc: 0.655000\n",
      "Epoch:  2/ 5    Batch: 100   Batch loss: 0.487068    Val loss: 0.592723 Val acc: 0.677500\n",
      "Epoch:  2/ 5    Batch: 105   Batch loss: 0.591973    Val loss: 0.625909 Val acc: 0.632500\n",
      "Epoch:  2/ 5    Batch: 110   Batch loss: 0.425671    Val loss: 0.591530 Val acc: 0.685000\n",
      "Epoch:  2/ 5    Batch: 115   Batch loss: 0.468000    Val loss: 0.584691 Val acc: 0.707500\n",
      "Epoch:  2/ 5    Batch: 120   Batch loss: 0.585555    Val loss: 0.773296 Val acc: 0.552500\n",
      "Epoch:  2/ 5    Batch: 125   Batch loss: 0.512674    Val loss: 0.596585 Val acc: 0.680000\n",
      "104\n",
      "Finished epoch 2. Average batch loss: 0.5768932444043458. Average validation loss: 0.63859448954463\n",
      "Starting epoch 3\n",
      "Epoch:  3/ 5    Batch: 130   Batch loss: 0.748074    Val loss: 0.597896 Val acc: 0.685000\n",
      "Epoch:  3/ 5    Batch: 135   Batch loss: 0.491664    Val loss: 0.595075 Val acc: 0.697500\n",
      "Epoch:  3/ 5    Batch: 140   Batch loss: 0.401920    Val loss: 0.846256 Val acc: 0.565000\n",
      "Epoch:  3/ 5    Batch: 145   Batch loss: 0.492336    Val loss: 0.587703 Val acc: 0.702500\n",
      "Epoch:  3/ 5    Batch: 150   Batch loss: 0.409437    Val loss: 0.639128 Val acc: 0.682500\n",
      "Epoch:  3/ 5    Batch: 155   Batch loss: 0.440972    Val loss: 0.647635 Val acc: 0.675000\n",
      "Epoch:  3/ 5    Batch: 160   Batch loss: 0.427859    Val loss: 0.716351 Val acc: 0.627500\n",
      "Epoch:  3/ 5    Batch: 165   Batch loss: 0.257529    Val loss: 0.585917 Val acc: 0.715000\n",
      "Epoch:  3/ 5    Batch: 170   Batch loss: 0.447018    Val loss: 0.574892 Val acc: 0.710000\n",
      "Epoch:  3/ 5    Batch: 175   Batch loss: 0.362525    Val loss: 0.677413 Val acc: 0.670000\n",
      "Epoch:  3/ 5    Batch: 180   Batch loss: 0.468480    Val loss: 0.561862 Val acc: 0.722500\n",
      "Epoch:  3/ 5    Batch: 185   Batch loss: 0.635923    Val loss: 0.575607 Val acc: 0.702500\n",
      "Epoch:  3/ 5    Batch: 190   Batch loss: 0.361530    Val loss: 0.862272 Val acc: 0.600000\n",
      "104\n",
      "Finished epoch 3. Average batch loss: 0.44285030057653785. Average validation loss: 0.651385215899119\n",
      "Starting epoch 4\n",
      "Epoch:  4/ 5    Batch: 195   Batch loss: 0.293396    Val loss: 0.570080 Val acc: 0.710000\n",
      "Epoch:  4/ 5    Batch: 200   Batch loss: 0.436107    Val loss: 0.588630 Val acc: 0.722500\n",
      "Epoch:  4/ 5    Batch: 205   Batch loss: 0.380544    Val loss: 0.596998 Val acc: 0.717500\n",
      "Epoch:  4/ 5    Batch: 210   Batch loss: 0.623843    Val loss: 0.585886 Val acc: 0.717500\n",
      "Epoch:  4/ 5    Batch: 215   Batch loss: 0.270947    Val loss: 0.599169 Val acc: 0.727500\n",
      "Epoch:  4/ 5    Batch: 220   Batch loss: 0.231376    Val loss: 0.589295 Val acc: 0.717500\n",
      "Epoch:  4/ 5    Batch: 225   Batch loss: 0.317937    Val loss: 0.578697 Val acc: 0.715000\n",
      "Epoch:  4/ 5    Batch: 230   Batch loss: 0.345304    Val loss: 0.656248 Val acc: 0.695000\n",
      "Epoch:  4/ 5    Batch: 235   Batch loss: 0.357956    Val loss: 0.598027 Val acc: 0.720000\n",
      "Epoch:  4/ 5    Batch: 240   Batch loss: 0.357242    Val loss: 0.575256 Val acc: 0.727500\n",
      "Epoch:  4/ 5    Batch: 245   Batch loss: 0.143588    Val loss: 0.602637 Val acc: 0.735000\n",
      "Epoch:  4/ 5    Batch: 250   Batch loss: 0.335362    Val loss: 0.585636 Val acc: 0.707500\n",
      "Epoch:  4/ 5    Batch: 255   Batch loss: 0.255581    Val loss: 0.620078 Val acc: 0.725000\n",
      "104\n",
      "Finished epoch 4. Average batch loss: 0.33406108035705984. Average validation loss: 0.595895138497536\n",
      "Starting epoch 5\n",
      "Epoch:  5/ 5    Batch: 260   Batch loss: 0.205162    Val loss: 0.640056 Val acc: 0.720000\n",
      "Epoch:  5/ 5    Batch: 265   Batch loss: 0.188250    Val loss: 0.611410 Val acc: 0.727500\n",
      "Epoch:  5/ 5    Batch: 270   Batch loss: 0.206379    Val loss: 0.627872 Val acc: 0.742500\n",
      "Epoch:  5/ 5    Batch: 275   Batch loss: 0.221950    Val loss: 0.617302 Val acc: 0.730000\n",
      "Epoch:  5/ 5    Batch: 280   Batch loss: 0.231467    Val loss: 0.608616 Val acc: 0.730000\n",
      "Epoch:  5/ 5    Batch: 285   Batch loss: 0.136017    Val loss: 0.630448 Val acc: 0.740000\n",
      "Epoch:  5/ 5    Batch: 290   Batch loss: 0.235295    Val loss: 0.621983 Val acc: 0.717500\n",
      "Epoch:  5/ 5    Batch: 295   Batch loss: 0.282836    Val loss: 0.607980 Val acc: 0.727500\n",
      "Epoch:  5/ 5    Batch: 300   Batch loss: 0.356837    Val loss: 0.661660 Val acc: 0.715000\n",
      "Epoch:  5/ 5    Batch: 305   Batch loss: 0.201363    Val loss: 0.616325 Val acc: 0.720000\n",
      "Epoch:  5/ 5    Batch: 310   Batch loss: 0.187999    Val loss: 0.602014 Val acc: 0.742500\n",
      "Epoch:  5/ 5    Batch: 315   Batch loss: 0.279285    Val loss: 0.594008 Val acc: 0.720000\n",
      "Epoch:  5/ 5    Batch: 320   Batch loss: 0.293637    Val loss: 0.614539 Val acc: 0.745000\n",
      "104\n",
      "Finished epoch 5. Average batch loss: 0.24512694729492068. Average validation loss: 0.6195548317180231\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate a model with pretrained embeddings with fine-tuning.\n",
    "# YOUR CODE HERE\n",
    "vocab_size = len(word_ids) + 1  # +1 for the 0 padding\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "embedding_model = EmbeddingLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, finetuning=True).to(DEVICE)\n",
    "reviews_train(embedding_model, train_loader, valid_loader, epochs=5, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2393de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.499\n",
      "Test accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "reviews_test(embedding_model, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0968e",
   "metadata": {},
   "source": [
    "## 2e Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b9183",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore different architectures and hyperparameters."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf2ba649d63ea02a221c132cc4497a7777afeaff3ccd7acc3a3ef0aa2b12ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mtp-ai-turing-tumble')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
