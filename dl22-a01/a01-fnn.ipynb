{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# IE 678 Deep Learning, University of Mannheim\n",
    "# Author: Rainer Gemulla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython import get_ipython\n",
    "from util import nextplot\n",
    "\n",
    "%matplotlib notebook\n",
    "get_ipython().magic('run -i \"a01-fnn-helper.py\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X1 (separable)\n",
    "nextplot()\n",
    "plot2(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X2 (not separable)\n",
    "nextplot()\n",
    "plot2(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_classify(X, w):\n",
    "    \"\"\"Classify using a perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch array of shape (N,D) or shape (D,)\n",
    "        Design matrix of test examples\n",
    "    w : torch array of shape (D,)\n",
    "        Weight vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch array of shape (N,)\n",
    "        Predicted binary labels (either 0 or 1)\"\"\"\n",
    "    if X.dim() == 1:\n",
    "        X = X.view(1, -1)\n",
    "    return (X @ w >= 0).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a+c Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_train(X, y, maxepochs=100, pocket=False, w0=None):\n",
    "    \"\"\"Train a perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch array of shape (N,D)\n",
    "        Design matrix\n",
    "    y : torch array of shape (N,)\n",
    "        Binary labels (either 0 or 1)\n",
    "    maxepochs : int\n",
    "        Maximum number of passes through the training set before the algorithm\n",
    "        returns\n",
    "    pocket : bool\n",
    "       Whether to use the pocket algorithm (True) or the perceptron learning algorithm\n",
    "       (False)\n",
    "    w0 : torch array of shape (D,)\n",
    "        Initial weight vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch array of shape (D,)\n",
    "        Fitted weight vector\"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "    if w0 is None:  # initial weight vector\n",
    "        w0 = torch.zeros(D)\n",
    "    w = w0  # current weight vector\n",
    "\n",
    "    ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b+d Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron using the perceptron learning algorithm and plot decision\n",
    "# boundary. You should get a perfect classification here. The decision boundary\n",
    "# should not change if you run this multiple times.\n",
    "w = pt_train(X1, y1)\n",
    "nextplot()\n",
    "plot2(X1, y1)\n",
    "plot2db(w, label=\"perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron using the pocket algorithm and plot decision boundary. You\n",
    "# should get a perfect classification here (with high probability). The decision\n",
    "# boundary should change if you run this multiple times.\n",
    "w = pt_train(X1, y1, pocket=True)\n",
    "nextplot()\n",
    "plot2(X1, y1)\n",
    "plot2db(w, label=\"pocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 10 perceptrons starting with random weights. Also train logistic\n",
    "# regression and SVM. Plot all decision boundaries, and print the\n",
    "# misclassification rates (on training data). Try this with and without the\n",
    "# pocket algorithm.\n",
    "nextplot()\n",
    "plot2dbs(X2, y2, n=10, maxepochs=1000, pocket=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi-Layer Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a Conjecture how an FNN fit will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the one-dimensional dataset that we will use\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Train with 2 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code. You do not need to modify this code.\n",
    "train_bfgs = lambda model, **kwargs: train_scipy(X3, y3, model, **kwargs)\n",
    "\n",
    "\n",
    "def train3(\n",
    "    hidden_sizes, nreps=10, transfer=lambda: nn.Sigmoid(), train=train_bfgs, **kwargs\n",
    "):\n",
    "    \"\"\"Train an FNN.\n",
    "\n",
    "    hidden_sizes is a (possibly empty) list containing the sizes of the hidden layer(s).\n",
    "    nreps refers to the number of repetitions.\n",
    "\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_cost = math.inf\n",
    "    for rep in range(nreps):\n",
    "        model = fnn_model([1] + hidden_sizes + [1], transfer)\n",
    "        print(f\"Repetition {rep: 2d}: \", end=\"\")\n",
    "        model = train(model, **kwargs)\n",
    "        mse = F.mse_loss(y3, model(X3)).item()\n",
    "        if mse < best_cost:\n",
    "            best_model = model\n",
    "            best_cost = mse\n",
    "        print(f\"best_cost={best_cost:.3f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the model with one hidden layer consisting of 2 units.\n",
    "model = train3([2], nreps=1)\n",
    "print(\"Training error:\", F.mse_loss(y3, model(X3)).item())\n",
    "print(\"Test error    :\", F.mse_loss(y3test, model(X3test)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and the fit\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight matrices and bias vectors can be read out as follows. If you want,\n",
    "# use these parameters to compute the output of the network (on X3) directly and\n",
    "# compare to model(X3).\n",
    "for par, value in model.state_dict().items():\n",
    "    print(f\"{par:<15}= {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now repeat this multiple times\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, always train multiple times (nreps=10 by default) and\n",
    "# report best model.\n",
    "model = train3([2], nreps=10)\n",
    "print(\"Training error:\", F.mse_loss(y3, model(X3)).item())\n",
    "print(\"Test error    :\", F.mse_loss(y3test, model(X3test)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different hidden layer sizes. To avoid recomputing\n",
    "# models, you may want to save your models using torch.save(model, filename) and\n",
    "# load them again using torch.load(filename).\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d Distributed representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model to analyze\n",
    "model = train3([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fit as well as the outputs of each neuron in the hidden\n",
    "# layer (scale for the latter is shown on right y-axis)\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), model, hidden=True, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fit as well as the outputs of each neuron in the hidden layer, scaled\n",
    "# by its weight for the output neuron (scale for the latter is shown on right\n",
    "# y-axis)\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), model, hidden=True, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e Experiment with different optimizers (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides many gradient-based optimizers; see\n",
    "# https://pytorch.org/docs/stable/optim.html. You can use a PyTorch optimizer\n",
    "# as follows.\n",
    "train_adam = lambda model, **kwargs: fnn_train(\n",
    "    X3, y3, model, optimizer=torch.optim.Adam(model.parameters(), lr=0.01), **kwargs\n",
    ")\n",
    "model = train3([50], nreps=1, train=train_adam, max_epochs=5000, tol=1e-8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different number of layers and activation functions. Here is\n",
    "# an example with three hidden layers (of sizes 4, 5, and 6) and ReLU activations.\n",
    "#\n",
    "# You can also plot the outputs of the hidden neurons in the first layer (using\n",
    "# the same code above).\n",
    "model = train3([4, 5, 6], nreps=50, transfer=lambda: nn.ReLU())\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), model)\n",
    "print(\"Training error:\", F.mse_loss(y3, model(X3)).item())\n",
    "print(\"Test error    :\", F.mse_loss(y3test, model(X3test)).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
